# Recurrent Neural Network: a comparison between LSTM and GRU
> 

## Introduction
This project is born for the Laboratory of Machine Learning, which is given as an add-on to the Fundamentals of Machine Learning course in the Master’s Degree in Informatics Engineering at the University of Florence.

The aim of this project is to go into detail of the Recurrent Neural Networks, which was one of the final leftover topics of the base course, and try an implementation of the most common types of RNN: the LSTM and the GRU, with the ultimate goal to decide which one performs better.


## Technologies and Launch
This code it’s written in Python, and uses Tensorflow and the Keras libraries for the specific deep learning functionalities.

It’s strongly recommended to launch it using a GPU, to keep the execution time reasonable. If you don’t have one, you can use Google Colab, that is the environment in which this project has been developed.


## Screenshots
<img width="1094" alt="Schermata 2023-03-16 alle 11 00 32" src="https://user-images.githubusercontent.com/126768526/225590864-e17c2efb-82c3-4542-a545-1490a07d7f85.png">


## Setup and Usage
The files in this repository are in .ipynb format so they are Jupyter Notebooks, and you can simply open them by importing in Google Colab. 

In the first file you’ll find the comparison between LSTM and GRU with step by step comments to each cell of code. 
In the second file you’ll find a useful pipeline version of the two implementations to reproduce the results by yourself. 
I’ll add to the repo also the .csv file containing the data used for the train of these networks.

